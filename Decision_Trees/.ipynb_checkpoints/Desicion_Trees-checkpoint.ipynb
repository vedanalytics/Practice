{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"body\" class=\"MissionLearn__section___3Mtj9 MissionMarkup__root___3FAB8\"><div><p>Decision trees are a powerful and popular machine learning technique.  The basic concept is very similar to trees you may have seen commonly used to aid decision-making.  Here's an example:</p>\n",
    "<div><diagram><svg class=\"diagram\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\" height=\"448\" width=\"368\"><g transform=\"translate(8,16 )\">\n",
    "<path d=\"M 0,192 L 0,240 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 32,144 L 32,192 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 40,32 L 40,96 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 72,192 L 72,240 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 112,96 L 112,144 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 120,352 L 120,400 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 152,304 L 152,352 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 168,208 L 168,256 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 192,32 L 192,96 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 200,352 L 200,400 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 208,144 L 208,208 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 224,256 L 224,304 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 264,352 L 264,400 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 288,208 L 288,256 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 304,304 L 304,352 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 352,352 L 352,400 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 40,32 L 192,32 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 40,96 L 192,96 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 32,144 L 208,144 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 0,192 L 72,192 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 168,208 L 288,208 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 0,240 L 72,240 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 168,256 L 288,256 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 152,304 L 304,304 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 120,352 L 200,352 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 264,352 L 352,352 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 120,400 L 200,400 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<path d=\"M 264,400 L 352,400 \" style=\"fill:none;stroke:#000;\"></path>\n",
    "<g transform=\"translate(0,0)\"><text text-anchor=\"middle\" x=\"0\" y=\"20\" style=\"fill:#000\">S</text><text text-anchor=\"middle\" x=\"8\" y=\"20\" style=\"fill:#000\">h</text><text text-anchor=\"middle\" x=\"16\" y=\"20\" style=\"fill:#000\">o</text><text text-anchor=\"middle\" x=\"24\" y=\"20\" style=\"fill:#000\">u</text><text text-anchor=\"middle\" x=\"32\" y=\"20\" style=\"fill:#000\">l</text><text text-anchor=\"middle\" x=\"40\" y=\"20\" style=\"fill:#000\">d</text><text text-anchor=\"middle\" x=\"56\" y=\"20\" style=\"fill:#000\">I</text><text text-anchor=\"middle\" x=\"72\" y=\"20\" style=\"fill:#000\">w</text><text text-anchor=\"middle\" x=\"80\" y=\"20\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"88\" y=\"20\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"96\" y=\"20\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"104\" y=\"20\" style=\"fill:#000\">t</text><text text-anchor=\"middle\" x=\"112\" y=\"20\" style=\"fill:#000\">l</text><text text-anchor=\"middle\" x=\"120\" y=\"20\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"136\" y=\"20\" style=\"fill:#000\">t</text><text text-anchor=\"middle\" x=\"144\" y=\"20\" style=\"fill:#000\">h</text><text text-anchor=\"middle\" x=\"152\" y=\"20\" style=\"fill:#000\">i</text><text text-anchor=\"middle\" x=\"160\" y=\"20\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"176\" y=\"20\" style=\"fill:#000\">b</text><text text-anchor=\"middle\" x=\"184\" y=\"20\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"192\" y=\"20\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"200\" y=\"20\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"208\" y=\"20\" style=\"fill:#000\">?</text><text text-anchor=\"middle\" x=\"56\" y=\"52\" style=\"fill:#000\">I</text><text text-anchor=\"middle\" x=\"64\" y=\"52\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"80\" y=\"52\" style=\"fill:#000\">t</text><text text-anchor=\"middle\" x=\"88\" y=\"52\" style=\"fill:#000\">h</text><text text-anchor=\"middle\" x=\"96\" y=\"52\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"112\" y=\"52\" style=\"fill:#000\">b</text><text text-anchor=\"middle\" x=\"120\" y=\"52\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"128\" y=\"52\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"136\" y=\"52\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"56\" y=\"68\" style=\"fill:#000\">l</text><text text-anchor=\"middle\" x=\"64\" y=\"68\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"72\" y=\"68\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"80\" y=\"68\" style=\"fill:#000\">g</text><text text-anchor=\"middle\" x=\"88\" y=\"68\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"96\" y=\"68\" style=\"fill:#000\">?</text><text text-anchor=\"middle\" x=\"56\" y=\"132\" style=\"fill:#000\">N</text><text text-anchor=\"middle\" x=\"64\" y=\"132\" style=\"fill:#000\">o</text><text text-anchor=\"middle\" x=\"152\" y=\"132\" style=\"fill:#000\">Y</text><text text-anchor=\"middle\" x=\"160\" y=\"132\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"168\" y=\"132\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"8\" y=\"212\" style=\"fill:#000\">W</text><text text-anchor=\"middle\" x=\"16\" y=\"212\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"24\" y=\"212\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"32\" y=\"212\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"40\" y=\"212\" style=\"fill:#000\">t</text><text text-anchor=\"middle\" x=\"48\" y=\"212\" style=\"fill:#000\">l</text><text text-anchor=\"middle\" x=\"56\" y=\"212\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"176\" y=\"228\" style=\"fill:#000\">C</text><text text-anchor=\"middle\" x=\"184\" y=\"228\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"192\" y=\"228\" style=\"fill:#000\">n</text><text text-anchor=\"middle\" x=\"208\" y=\"228\" style=\"fill:#000\">I</text><text text-anchor=\"middle\" x=\"224\" y=\"228\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"232\" y=\"228\" style=\"fill:#000\">u</text><text text-anchor=\"middle\" x=\"240\" y=\"228\" style=\"fill:#000\">n</text><text text-anchor=\"middle\" x=\"176\" y=\"244\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"184\" y=\"244\" style=\"fill:#000\">w</text><text text-anchor=\"middle\" x=\"192\" y=\"244\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"200\" y=\"244\" style=\"fill:#000\">y</text><text text-anchor=\"middle\" x=\"208\" y=\"244\" style=\"fill:#000\">?</text><text text-anchor=\"middle\" x=\"176\" y=\"292\" style=\"fill:#000\">N</text><text text-anchor=\"middle\" x=\"184\" y=\"292\" style=\"fill:#000\">o</text><text text-anchor=\"middle\" x=\"256\" y=\"292\" style=\"fill:#000\">Y</text><text text-anchor=\"middle\" x=\"264\" y=\"292\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"272\" y=\"292\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"128\" y=\"372\" style=\"fill:#000\">W</text><text text-anchor=\"middle\" x=\"136\" y=\"372\" style=\"fill:#000\">r</text><text text-anchor=\"middle\" x=\"144\" y=\"372\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"152\" y=\"372\" style=\"fill:#000\">s</text><text text-anchor=\"middle\" x=\"160\" y=\"372\" style=\"fill:#000\">t</text><text text-anchor=\"middle\" x=\"168\" y=\"372\" style=\"fill:#000\">l</text><text text-anchor=\"middle\" x=\"176\" y=\"372\" style=\"fill:#000\">e</text><text text-anchor=\"middle\" x=\"272\" y=\"372\" style=\"fill:#000\">R</text><text text-anchor=\"middle\" x=\"280\" y=\"372\" style=\"fill:#000\">u</text><text text-anchor=\"middle\" x=\"288\" y=\"372\" style=\"fill:#000\">n</text><text text-anchor=\"middle\" x=\"304\" y=\"372\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"312\" y=\"372\" style=\"fill:#000\">w</text><text text-anchor=\"middle\" x=\"320\" y=\"372\" style=\"fill:#000\">a</text><text text-anchor=\"middle\" x=\"328\" y=\"372\" style=\"fill:#000\">y</text></g></g></svg></diagram></div>\n",
    "<p>In the diagram above, we're deciding whether we should wrestle a bear that's in front of us.  We're using various criteria to make our final decision, including the size of the bear, and whether escape is possible.  Let's say we had a data set of people who survived bear encounters and the actions they took:</p>\n",
    "</div><div class=\"MissionMarkup__code_block___2vANs DqEditor__root___2C-3R\"><textarea autocomplete=\"off\" style=\"display: none;\">Bear name    Size    Escape possible?    Action\n",
    "Yogi         Small   No                  Wrestle\n",
    "Winnie       Small   Yes                 Wrestle\n",
    "Baloo        Large   Yes                 Run away\n",
    "Gentle Ben   Large   No                  Wrestle</textarea><div class=\"CodeMirror cm-s-dq-light CodeMirror-wrap\"><div style=\"overflow: hidden; position: relative; width: 3px; height: 0px; top: 43.5px; left: 301px;\"><textarea autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" style=\"position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;\" tabindex=\"0\"></textarea></div><div class=\"CodeMirror-vscrollbar\" cm-not-content=\"true\"><div style=\"min-width: 1px; height: 0px;\"></div></div><div class=\"CodeMirror-hscrollbar\" cm-not-content=\"true\"><div style=\"height: 100%; min-height: 1px; width: 0px;\"></div></div><div class=\"CodeMirror-scrollbar-filler\" cm-not-content=\"true\"></div><div class=\"CodeMirror-gutter-filler\" cm-not-content=\"true\"></div><div class=\"CodeMirror-scroll\" tabindex=\"-1\"><div class=\"CodeMirror-sizer\" style=\"margin-left: 0px; margin-bottom: -17px; border-right-width: 13px; min-height: 109px; padding-right: 0px; padding-bottom: 0px;\"><div style=\"position: relative; top: 0px;\"><div class=\"CodeMirror-lines\" role=\"presentation\"><div role=\"presentation\" style=\"position: relative; outline: none;\"><div class=\"CodeMirror-measure\"><pre>x</pre></div><div class=\"CodeMirror-measure\"></div><div style=\"position: relative; z-index: 1;\"></div><div class=\"CodeMirror-cursors\" style=\"visibility: hidden;\"><div class=\"CodeMirror-cursor\" style=\"left: 301px; top: 36px; height: 18px;\">&nbsp;</div></div><div class=\"CodeMirror-code\" role=\"presentation\" style=\"\"><pre class=\" CodeMirror-line \" role=\"presentation\"><span role=\"presentation\" style=\"padding-right: 0.1px;\"><span class=\"cm-variable\">Bear</span> <span class=\"cm-variable\">name</span> &nbsp; &nbsp;<span class=\"cm-variable\">Size</span> &nbsp; &nbsp;<span class=\"cm-variable\">Escape</span> <span class=\"cm-variable\">possible</span><span class=\"cm-error\">?</span> &nbsp; &nbsp;<span class=\"cm-variable\">Action</span></span></pre><pre class=\" CodeMirror-line \" role=\"presentation\"><span role=\"presentation\" style=\"padding-right: 0.1px;\"><span class=\"cm-variable\">Yogi</span> &nbsp; &nbsp; &nbsp; &nbsp; <span class=\"cm-variable\">Small</span> &nbsp; <span class=\"cm-variable\">No</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class=\"cm-variable\">Wrestle</span></span></pre><pre class=\" CodeMirror-line \" role=\"presentation\"><span role=\"presentation\" style=\"padding-right: 0.1px;\"><span class=\"cm-variable\">Winnie</span> &nbsp; &nbsp; &nbsp; <span class=\"cm-variable\">Small</span> &nbsp; <span class=\"cm-variable\">Yes</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span class=\"cm-variable\">Wrestle</span></span></pre><pre class=\" CodeMirror-line \" role=\"presentation\"><span role=\"presentation\" style=\"padding-right: 0.1px;\"><span class=\"cm-variable\">Baloo</span> &nbsp; &nbsp; &nbsp; &nbsp;<span class=\"cm-variable\">Large</span> &nbsp; <span class=\"cm-variable\">Yes</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span class=\"cm-variable\">Run</span> <span class=\"cm-variable\">away</span></span></pre><pre class=\" CodeMirror-line \" role=\"presentation\"><span role=\"presentation\" style=\"padding-right: 0.1px;\"><span class=\"cm-variable\">Gentle</span> <span class=\"cm-variable\">Ben</span> &nbsp; <span class=\"cm-variable\">Large</span> &nbsp; <span class=\"cm-variable\">No</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class=\"cm-variable\">Wrestle</span></span></pre></div></div></div></div></div><div style=\"position: absolute; height: 13px; width: 1px; border-bottom: 0px solid transparent; top: 109px;\"></div><div class=\"CodeMirror-gutters\" style=\"display: none; height: 122px;\"></div></div></div></div><div>\n",
    "<p>If we wanted to optimize our chances of surviving a bear encounter, we could construct a decision tree to tell us what action to take.</p>\n",
    "<p>As our data set gets larger though, this becomes less and less practical.  What if we had 10000 rows and 10 variables?  Would you want to look through all of the possibilities to construct a tree?</p>\n",
    "<p>This is where the decision tree machine learning algorithm can help.  It enables us to automatically construct a decision tree that tells us what outcomes we should predict in certain situations.  </p>\n",
    "<p>The decision tree algorithm is a supervised learning algorithm -- we first construct the tree with historical data, and then use it to predict an outcome.  One of the major advantages of decision trees is that they can pick up nonlinear interactions between variables in the data that linear regression can't.  In our bear wrestling example, a decision tree can pick up on the fact that you should only wrestle large bears when escape is impossible, whereas a linear regression would have had to weight both factors in the absence of the other.</p>\n",
    "<p>We can use trees for classification or regression problems.  In this lesson, we'll walk through the building blocks of making a decision tree automatically.</p></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be looking at individual income in the United States. The data is from the 1994 census, and contains information on an individual's marital status, age, type of work, and more. The target column, or what we want to predict, is whether individuals make less than or equal to 50k a year, or more than 50k a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education_num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital_status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital_gain  capital_loss  hours_per_week  native_country high_income  \n",
      "0          2174             0              40   United-States       <=50K  \n",
      "1             0             0              13   United-States       <=50K  \n",
      "2             0             0              40   United-States       <=50K  \n",
      "3             0             0              40   United-States       <=50K  \n",
      "4             0             0              40            Cuba       <=50K  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# Set index_col to False to avoid pandas thinking that the first column is row indexes (it's age)\n",
    "income = pandas.read_csv(\"income.csv\", index_col=False)\n",
    "print(income.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in our data, we have categorical variables such as workclass that have string values. Multiple individuals can share the same string value. The types of work include State-gov, Self-emp-not-inc, Private, and so on. Each of these strings is a label for a category. Another example of a column of categories is sex, where the options are Male and Female.\n",
    "\n",
    "Before we get started with decision trees, we need to convert the categorical variables in our data set to numeric variables. This involves assigning a number to each category label, then converting all of the labels in a column to the corresponding numbers.\n",
    "\n",
    "One strategy is to convert the columns to a categorical type. Under this approach, pandas will display the labels as strings, but internally store them as numbers so we can do computations with them. The numbers aren't always compatible with other libraries like Scikit-learn, though, so it's easier to just do the conversion to numeric upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting categorical column to numeric\n",
    "col = pandas.Categorical(income['workclass'])\n",
    "income['workclass'] = col.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting remaining categorical columns to numeric\n",
    "for name in [\"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\", \"high_income\"]:\n",
    "    col = pandas.Categorical(income[name])\n",
    "    income[name] = col.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is made up of a series of nodes and branches. A node is where we split the data based on a variable, and a branch is one side of the split. Here's an example:\n",
    "\n",
    "W\n",
    "h\n",
    "a\n",
    "t\n",
    "i\n",
    "n\n",
    "c\n",
    "o\n",
    "m\n",
    "e\n",
    "d\n",
    "o\n",
    "p\n",
    "e\n",
    "o\n",
    "p\n",
    "l\n",
    "e\n",
    "m\n",
    "a\n",
    "k\n",
    "e\n",
    "?\n",
    "D\n",
    "o\n",
    "t\n",
    "h\n",
    "e\n",
    "y\n",
    "w\n",
    "o\n",
    "r\n",
    "k\n",
    "i\n",
    "n\n",
    "t\n",
    "h\n",
    "e\n",
    "P\n",
    "r\n",
    "i\n",
    "v\n",
    "a\n",
    "t\n",
    "e\n",
    "s\n",
    "e\n",
    "c\n",
    "t\n",
    "o\n",
    "r\n",
    "?\n",
    "N\n",
    "o\n",
    "Y\n",
    "e\n",
    "s\n",
    "In the diagram above, the node splits the data into two branches, No and Yes, based on whether the individual works in the private sector (the workclass column). We've mapped the value Private in the workclass column to the numeric code 4 (we can check this by comparing the values in the workclass column that used to have the label Private with the current values to see where they line up). So the No branch corresponds to workclass != 4, and the Yes branch corresponds to workclass == 4.\n",
    "\n",
    "This is exactly how a decision tree works -- we keep splitting the data based on variables. As we do this, the tree accumulates more levels. The tree we made above is two levels deep because it has one split, and two \"levels\" of nodes.\n",
    "\n",
    "The tree below is three levels deep.\n",
    "\n",
    "W\n",
    "h\n",
    "a\n",
    "t\n",
    "i\n",
    "n\n",
    "c\n",
    "o\n",
    "m\n",
    "e\n",
    "d\n",
    "o\n",
    "p\n",
    "e\n",
    "o\n",
    "p\n",
    "l\n",
    "e\n",
    "m\n",
    "a\n",
    "k\n",
    "e\n",
    "?\n",
    "D\n",
    "o\n",
    "t\n",
    "h\n",
    "e\n",
    "y\n",
    "w\n",
    "o\n",
    "r\n",
    "k\n",
    "i\n",
    "n\n",
    "t\n",
    "h\n",
    "e\n",
    "P\n",
    "r\n",
    "i\n",
    "v\n",
    "a\n",
    "t\n",
    "e\n",
    "s\n",
    "e\n",
    "c\n",
    "t\n",
    "o\n",
    "r\n",
    "?\n",
    "N\n",
    "o\n",
    "Y\n",
    "e\n",
    "s\n",
    "N\n",
    "a\n",
    "t\n",
    "i\n",
    "v\n",
    "e\n",
    "t\n",
    "o\n",
    "t\n",
    "h\n",
    "e\n",
    "U\n",
    "S\n",
    "?\n",
    "N\n",
    "o\n",
    "Y\n",
    "e\n",
    "s\n",
    "We added another split point to this tree \"below\", or after, our first split point. This made the tree three levels deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now think of rows of data flowing through a decision tree. In the diagram above, we can split the data set into two portions based on whether the individual works in the private sector or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22696, 15)\n",
      "(9865, 15)\n"
     ]
    }
   ],
   "source": [
    "# Enter your code here\n",
    "private_incomes = income[income[\"workclass\"] == 4]\n",
    "public_incomes = income[income[\"workclass\"] != 4]\n",
    "\n",
    "print(private_incomes.shape)\n",
    "print(public_incomes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we performed the split, 9865 rows went to the left, where workclass does not equal 4, and 22696 rows went to the right, where workclass equals 4.\n",
    "\n",
    "It's useful to think of a decision tree as a flow of rows of data. When we make a split, some rows will go to the right, and some will go to the left. As we build the tree deeper and deeper, each node will \"receive\" fewer and fewer rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes at the bottom of the tree, where we decide to stop splitting, are called terminal nodes, or leaves. When we do our splits, we aren't doing them randomly; we have an objective. Our goal is to ensure that we can make a prediction on future data. In order to do this, all rows in each leaf must have only one value for our target column.\n",
    "\n",
    "We're trying to predict the high_income column.\n",
    "If high_income is 1, it means that the person has an income higher than 50k a year.\n",
    "If high_income is 0, it means that they have an income less than or equal to 50k a year.\n",
    "\n",
    "After constructing a tree using the income data, we'll want to make predictions. In order to do this, we'll take a new row and feed it through our decision tree.\n",
    "\n",
    "First, we check whether the person works in the private sector. If they do, we'll check to see whether they're native to the US, and so on.\n",
    "\n",
    "Eventually, we'll reach a leaf. The leaf will tell us what value we should predict for high_income.\n",
    "\n",
    "In order to be able to make this prediction, all of the rows in a leaf should only have a single value for high_income. This means that leaves can't have both 0 and 1 values in the high_income column. Each leaf can only have rows with the same values for our target column. If this isn't the case, we won't be able to make effective predictions.\n",
    "\n",
    "We'll need to continue splitting nodes until we get to a point where all of the rows in a node have the same value for high_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a high-level view of how decision trees work, let's explore the details and learn how to perform the splits.\n",
    "\n",
    "We'll use a specific measure to figure out which variables we should split nodes on. Post-split, we'll have two data sets, each containing the rows from one branch of the split.\n",
    "\n",
    "Because we're trying to reach the leaves having only 1s or only 0s in high_income, each split will need to get us closer to that goal.\n",
    "\n",
    "When we split, we'll try to separate as many 0s from 1s in the high_income column as we can. In order to do this, we need a metric for how \"together\" the different values in the high_income column are.\n",
    "\n",
    "Data scientists commonly use a metric called entropy for this purpose. Entropy refers to disorder. The more \"mixed together\" 1s and 0s are, the higher the entropy. A data set consisting entirely of 1s in the high_income column would have low entropy.\n",
    "\n",
    "Entropy, which is not to be confused with entropy from physics, comes from information theory. Information theory is based on probability and statistics, and deals with the transmission, processing, utilization, and extraction of information. A key concept in information theory is the notion of a bit of information. One bit of information is one unit of information.\n",
    "\n",
    "We can represent a bit of information as a binary number because it either has the value 1 or 0. Suppose there's an equal probability of tomorrow being sunny (1) or not sunny (0). If I tell you that it will be sunny, I've given you one bit of information.\n",
    "\n",
    "We can also think of entropy in terms of information. If we flip a coin where both sides are heads, we know upfront that the result will be heads. We gain no new information by flipping the coin, so entropy is 0. On the other hand, if the coin has a heads side and a tails side, there's a 50% probability that it will land on either. Thus, flipping the coin gives us one bit of information -- which side the coin landed on.\n",
    "\n",
    "Entropy can be much more complex, especially when we get to cases with more than two possible outcomes, or differential probabilities. A deep understanding of entropy isn't necessary for constructing decision trees, however. If you'd like, you can read more about entropy at Wikipedia.\n",
    "\n",
    "The formula for entropy looks like this:\n",
    "\n",
    "\\begin{equation*}-\\sum_{i=1}^{c} {\\mathrm{P}(x_i) \\log_b \\mathrm{P}(x_i)}\\end{equation*}\n",
    "We iterate through each unique value in a single column (in this case, high_income), and assign it to i. We then compute the probability of that value occurring in the data (P(xi)). Next we do some multiplication, and sum all of the values together. b is the base of the logarithm. We commonly use the value 2 for this, but we can also set it to 10 or another value.\n",
    "\n",
    "Let's say we have this data:\n",
    "\n",
    "\n",
    "    age    high_income\n",
    "    25     1\n",
    "    50     1\n",
    "    30     0\n",
    "    50     0\n",
    "    80     1\n",
    "We could compute its entropy like this:\n",
    "\n",
    "\\begin{equation*}-\\sum_{i=1}^{c} {\\mathrm{P}(x_i) \\log_b \\mathrm{P}(x_i)} \\nonumber \\\\ -((2/5 * \\log_2 2/5) + (3/5 * \\log_2 3/5)) \\nonumber \\\\ -(-0.5287712379549449 + -0.44217935649972373)\\nonumber \\\\ .97 \\nonumber \\\\ \\end{equation*}\n",
    "We get less than one \"bit\" of information -- only .97 -- because there are slightly more 1s in the sample data than 0s. This means that if we were predicting a new value, we could guess that the answer is 1 and be right more often than wrong (because there's a .6 probability of the answer being 1). Due to this prior knowledge, we gain less than a full \"bit\" of information when we observe a new value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_0 = income[income[\"high_income\"] == 0].shape[0] / income.shape[0]\n",
    "prob_1 = income[income[\"high_income\"] == 1].shape[0] / income.shape[0]\n",
    "income_entropy = -(prob_0 * math.log(prob_0, 2) + prob_1 * math.log(prob_1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a way to go from computing entropy to figuring out which variable to split on. We can do this using information gain, which tells us which split will reduce entropy the most.\n",
    "\n",
    "Here's the formula for information gain:\n",
    "\\begin{equation*}IG(T,A) = Entropy(T)-\\sum_{v\\in A}\\frac{|T_{v}|}{|T|} \\cdot Entropy(T_{v})\\end{equation*}\n",
    "\n",
    "It may look complicated, but we'll break it down. We're computing information gain (IG) for a given target variable (T), as well as a given variable we want to split on (A).\n",
    "\n",
    "To compute it, we first calculate the entropy for T. Then, for each unique value v in the variable A, we compute the number of rows in which A takes on the value v, and divide it by the total number of rows. Next, we multiply the results by the entropy of the rows where A is v. We add all of these subset entropies together, then subtract from the overall entropy to get information gain.\n",
    "\n",
    "Here's an alternate explanation. We're finding the entropy of each set post-split, weighting it by the number of items in each split, then subtracting from the current entropy. If the result is positive, we've lowered entropy with our split. The higher the result is, the more we've lowered entropy.\n",
    "\n",
    "One strategy for constructing trees is to create as many branches at each node as there are unique values for the variable we're splitting on. So if the variable has three or four values, we'd end up with three or four branches. This approach usually involves more complexity than it's worth and doesn't improve prediction accuracy, but it's worth knowing about.\n",
    "\n",
    "To simplify the calculation of information gain and make splits simpler, we won't do it for each unique value. We'll find the median for the variable we're splitting on instead. Any rows where the value of the variable is below the median will go to the left branch, and the rest of the rows will go to the right branch. To compute information gain, we'll only have to compute entropies for two subsets.\n",
    "\n",
    "Here's an example that uses the same data set we worked with earlier:\n",
    "\n",
    "    age    high_income\n",
    "    25     1\n",
    "    50     1\n",
    "    30     0\n",
    "    50     0\n",
    "    80     1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to split this data set based on age. First, we calculate the median age, which is 50. Then, we assign any row with a value less than or equal to the median age the value 0 (in a new column named split_age), and the other rows 1.\n",
    "    age    high_income    split_age\n",
    "    25     1              0\n",
    "    50     1              0\n",
    "    30     0              0\n",
    "    50     0              0\n",
    "    80     1              1\n",
    "\n",
    "Now we compute entropy:\n",
    "\\begin{equation} IG(T,A) = Entropy(T)-\\sum_{v\\in A}\\frac{|T_{v}|}{|T|} \\cdot Entropy(T_{v}) \\nonumber \\\\ .97 - (((4/5) * -(1/2 * log_{2} 1/2 + 1/2 * log_{2} 1/2 )) + -(1/5 * (0 * log_{2} 0 + 1 * log_{2} 1 ))) \\nonumber \\\\ .97 - ((4/5) * -(-.5 + -.5)) + (1/5 * -(0 + 1 * 0 )) \\nonumber \\\\ .97 - (4/5) \\nonumber \\\\ .169 \\nonumber \\\\ \\end{equation}\n",
    "\n",
    "We end up with .17, which means that we gain .17 bits of information by splitting our data set on the age variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculating Information gain:\n",
    "Compute the information gain for splitting on the age column of income.\n",
    "\n",
    "First, compute the median of age.\n",
    "Then, assign anything less than or equal to the median to the left branch, and anything greater than the median to the right branch.\n",
    "Compute the information gain and assign it to age_information_gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970950594455\n",
      "0.170950594455\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def calc_entropy(column):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a pandas series, list, or numpy array.\n",
    "    \"\"\"\n",
    "    # Compute the counts of each unique value in the column\n",
    "    counts = numpy.bincount(column)\n",
    "    # Divide by the total column length to get a probability\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    # Initialize the entropy to 0\n",
    "    entropy = 0\n",
    "    # Loop through the probabilities, and add each one to the total entropy\n",
    "    for prob in probabilities:\n",
    "        if prob > 0:\n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    \n",
    "    return -entropy\n",
    "\n",
    "# Verify that our function matches our answer from earlier\n",
    "entropy = calc_entropy([1,1,0,0,1])\n",
    "print(entropy)\n",
    "\n",
    "information_gain = entropy - ((.8 * calc_entropy([1,1,0,0])) + (.2 * calc_entropy([1])))\n",
    "print(information_gain)\n",
    "income_entropy = calc_entropy(income[\"high_income\"])\n",
    "\n",
    "median_age = income[\"age\"].median()\n",
    "\n",
    "left_split = income[income[\"age\"] <= median_age]\n",
    "right_split = income[income[\"age\"] > median_age]\n",
    "\n",
    "age_information_gain = income_entropy - ((left_split.shape[0] / income.shape[0]) * calc_entropy(left_split[\"high_income\"]) + ((right_split.shape[0] / income.shape[0]) * calc_entropy(right_split[\"high_income\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to compute information gain, we can determine the best variable to split a node on. When we start our tree, we want to make an initial split. We'll find the variable to split on by calculating which split would have the highest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0470286613047\n"
     ]
    }
   ],
   "source": [
    "def calc_information_gain(data, split_name, target_name):\n",
    "    \"\"\"\n",
    "    Calculate information gain given a data set, column to split on, and target\n",
    "    \"\"\"\n",
    "    # Calculate the original entropy\n",
    "    original_entropy = calc_entropy(data[target_name])\n",
    "    \n",
    "    # Find the median of the column we're splitting\n",
    "    column = data[split_name]\n",
    "    median = column.median()\n",
    "    \n",
    "    # Make two subsets of the data, based on the median\n",
    "    left_split = data[column <= median]\n",
    "    right_split = data[column > median]\n",
    "    \n",
    "    # Loop through the splits and calculate the subset entropies\n",
    "    to_subtract = 0\n",
    "    for subset in [left_split, right_split]:\n",
    "        prob = (subset.shape[0] / data.shape[0]) \n",
    "        to_subtract += prob * calc_entropy(subset[target_name])\n",
    "    \n",
    "    # Return information gain\n",
    "    return original_entropy - to_subtract\n",
    "\n",
    "# Verify that our answer is the same as on the last screen\n",
    "print(calc_information_gain(income, \"age\", \"high_income\"))\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "information_gains = [calc_information_gain(income,column,\"high_income\") for column in columns]\n",
    "highest_gain_index = information_gains.index(max(information_gains))\n",
    "highest_gain = columns[highest_gain_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing Decision Tree using ID3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def id3(data, target, columns)\n",
    "    1 Create a node for the tree\n",
    "    2 If all values of the target attribute are 1, Return the node, with label = 1\n",
    "    3 If all values of the target attribute are 0, Return the node, with label = 0\n",
    "    4 Using information gain, find A, the column that splits the data best\n",
    "    5 Find the median value in column A\n",
    "    6 Split column A into values below or equal to the median (0), and values above the median (1)\n",
    "    7 For each possible value (0 or 1), vi, of A,\n",
    "    8    Add a new tree branch below Root that corresponds to rows of data where A = vi\n",
    "    9    Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "   10    Below this new branch add the subtree id3(data[A==vi], target, columns)\n",
    "   11 Return Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we need a function that returns the name of the column we should use to split a data set. The function should take the name of the data set, the target column, and a list of columns we might want to split on as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_column(data, target_name, columns):\n",
    "    # data is a dataframe\n",
    "    # target_name is the name of the target variable\n",
    "    # columns is a list of potential columns to split on\n",
    "    information_gains = [calc_information_gain(data,column,target_name) for column in columns]\n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    highest_gain = columns[highest_gain_index]\n",
    "    return highest_gain\n",
    "\n",
    "# A list of columns to potentially split income with\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "income_split = find_best_column(income,\"high_income\",columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build up to making the full id3() function by creating a simpler algorithm that we can extend. Here's what that algorithm looks like in pseudocode:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def id3(data, target, columns)\n",
    "    1 Create a node for the tree\n",
    "    2 If all values of the target attribute are 1, add 1 to counter_1\n",
    "    3 If all values of the target attribute are 0, add 1 to counter_0\n",
    "    4 Using information gain, find A, the column that splits the data best\n",
    "    5 Find the median value in column A\n",
    "    6 Split A into values below or equal to the median (0), and values above the median (1)\n",
    "    7 For each possible value (0 or 1), vi, of A,\n",
    "    8    Add a new tree branch below Root that corresponds to rows of data where A = vi\n",
    "    9    Let Examples(vi) be the subset of examples that have the value vi for A\n",
    "   10    Below this new branch, add the subtree id3(data[A==vi], target, columns)\n",
    "   11 Return Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is very similar to the algorithm above, but lines 2 and 3 are different. Rather than storing the entire tree (which is a bit complicated), we'll just tally how many leaves end up with the label 1, and how many end up with the label 0.\n",
    "\n",
    "We'll replicate this algorithm in code, and apply it to the same data set we just stepped through on a previous screen:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "high_income    age    marital_status\n",
    "0              20     0\n",
    "0              60     2\n",
    "0              40     1\n",
    "1              25     1\n",
    "1              35     2\n",
    "1              55     1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(data, target, columns):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            label_0s.append(0)\n",
    "        elif 1 in unique_targets:\n",
    "            label_1s.append(1)\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    \n",
    "    for split in [left_split, right_split]:\n",
    "        id3(split, target, columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
